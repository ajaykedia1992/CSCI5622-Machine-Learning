{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Homework \n",
    "***\n",
    "**Name**: Ajay Kedia\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Wednesday May 2nd**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:42:13.096354Z",
     "start_time": "2018-04-20T02:42:12.091124Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 1: Building and Training a Feed-Forward Neural Network \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general feed-forward neural network class that utilizes sigmoid activation functions. Your tasks will be to implement `forward propagation`, `prediction`, `back propagation`, `gradient_checking`, and a general `train` routine to learn the weights in your network via Stochastic Gradient Descent.  \n",
    "\n",
    "The skeleton for the `Network` class is below. Note that this class is almost identical to the one you worked with in the **Lecture 18** in-class notebook, so you should look there to remind yourself of the details.   Scroll down to find more information about your tasks as well as unit tests. \n",
    "\n",
    "**Important Note**: In **Problem 2** we'll be using the `Network` class to train a network to do handwritten digit recognition.  Please make sure to utilize vectorized Numpy routines as much as possible, as writing inefficient code here will cause very slow training times in **Problem 2**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T03:16:00.042109Z",
     "start_time": "2018-04-20T03:15:59.375181Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def C(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate the cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        # TODO: Initialize activation on initial layer to x\n",
    "        \n",
    "        \n",
    "        self.a[0] = x\n",
    "        \n",
    "        \n",
    "        for k in range(self.L-1):\n",
    "            m = np.dot(self.W[k],self.a[k]) + self.b[k]\n",
    "            self.a[k+1] = self.g(m)\n",
    "            self.z[k+1] = m   \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "        \n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "        \n",
    "        # TODO: Populate yhat with one-hot-coded predictions\n",
    "        \n",
    "        for k, x in enumerate(X):\n",
    "            \n",
    "            #Callinf forward propogation\n",
    "            self.forward_prop(x)\n",
    "            \n",
    "            # vactorized y\n",
    "            yhat[k] = np.array([1 if a > 0.5 else 0 for a in self.a[-1]]) \n",
    "                \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations \n",
    "        \n",
    "        #Callinf forward propogation\n",
    "        self.forward_prop(x)\n",
    "        \n",
    "        # TODO: compute deltas on output layer \n",
    "        \n",
    "        # Calling derivative of sigmoid function and cost function\n",
    "        self.delta[-1] =  self.g_prime(self.z[-1]) * self.gradC(self.a[-1], y)\n",
    "        \n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for k in range(self.L-2, -1, -1):\n",
    "            t = self.delta[k + 1]\n",
    "            #activation function\n",
    "            p = self.a[k]\n",
    "            #transposing p\n",
    "            Transpose_p = p[:, np.newaxis].T\n",
    "            \n",
    "            # calculating derivatives of weight\n",
    "            self.dW[k] = np.dot(t[:, np.newaxis], Transpose_p)\n",
    "            \n",
    "            # calculating derivative of bias\n",
    "            self.db[k] = self.delta[k + 1]\n",
    "            \n",
    "            \n",
    "            self.delta[k] = self.g_prime(self.z[k]) * np.dot(self.W[k].T, self.delta[k + 1])\n",
    "            \n",
    "        \n",
    "    def gradient_checking(self, X_train, y_train, EPS=0.0001):\n",
    "        \"\"\"\n",
    "        Performs gradient checking on all weights in the \n",
    "        network for a randomly selected training example \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        \"\"\"\n",
    "        # Randomly select a training example \n",
    "        kk = np.random.randint(0,X_train.shape[0])\n",
    "        xk = X_train[kk]\n",
    "        yk = y_train[kk]\n",
    "\n",
    "        # Get the analytic(ish) weights from back_prop \n",
    "        self.back_prop(xk, yk)\n",
    "\n",
    "        # List of relative errors.  Used only for unit testing. \n",
    "        rel_errors = []\n",
    "\n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        # Don't forget that after perturbing the weights\n",
    "        # you'll want to put them back the way they were! \n",
    "        \n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        for ell in range(self.L-1):\n",
    "            for ii in range(self.W[ell].shape[0]):\n",
    "                # Check weights in level W[ell][ii,jj] \n",
    "                for jj in range(self.W[ell].shape[1]):\n",
    "                    \n",
    "                    # TODO true_dW  \n",
    "                    # TODO num_dW  \n",
    "                    \n",
    "                    # calculating old weight on level W[ell][ii,jj] \n",
    "                    old_Weight = self.W[ell][ii, jj]\n",
    "                    \n",
    "                    # calculating true weight on level W[ell][ii,jj] using weight derivative\n",
    "                    true_dW = self.dW[ell][ii, jj]\n",
    "                    \n",
    "                    #Updating weight by adding EPS\n",
    "                    \n",
    "                    self.W[ell][ii, jj] = self.W[ell][ii, jj] + EPS\n",
    "                    \n",
    "                    #calling forward propogation\n",
    "                    self.forward_prop(xk)\n",
    "                    \n",
    "                    one = self.C(self.a[-1], yk)\n",
    "                    \n",
    "                    #updating weight by subtracting EPS\n",
    "                    self.W[ell][ii, jj] = old_Weight - EPS\n",
    "                    \n",
    "                    # forward propogation\n",
    "                    self.forward_prop(xk)\n",
    "                    \n",
    "                    # after forwarding propogation calling cost function\n",
    "                    two = self.C(self.a[-1], yk)\n",
    "                    \n",
    "                    \n",
    "                    num_dW = (one - two) / (2 * EPS)\n",
    "                    \n",
    "                    # saving old weight back\n",
    "                    \n",
    "                    self.W[ell][ii, jj] = old_Weight\n",
    "                    \n",
    "                    rel_dW = np.abs(true_dW-num_dW)/np.abs(true_dW)\n",
    "                    print(\"W[{:d}][{:d},{:d}]: true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, jj, true_dW, num_dW, rel_dW))\n",
    "                    rel_errors.append(rel_dW)\n",
    "                    \n",
    "                # Check bias b[ell][ii]\n",
    "                \n",
    "                # TODO true_db  \n",
    "                # TODO num_db  \n",
    "                \n",
    "                # we will proceed the same way as weight\n",
    "                \n",
    "                # old bias value from b[ell][ii]\n",
    "                \n",
    "                old_bias_value = self.b[ell][ii]\n",
    "                \n",
    "                # true bias by using derivative of bias\n",
    "                true_db = self.db[ell][ii]\n",
    "                \n",
    "\n",
    "                # updating bias value adding existing and EPS\n",
    "                \n",
    "                self.b[ell][ii] = self.b[ell][ii] + EPS\n",
    "                \n",
    "                \n",
    "                # calling forward propogation\n",
    "                self.forward_prop(xk)\n",
    "                \n",
    "                # cost function after forward propogation and added EPS in bias\n",
    "                one = self.C(self.a[-1], yk)\n",
    "\n",
    "                # subtracting EPS from old bias value using EPS\n",
    "                self.b[ell][ii] = old_bias_value - EPS\n",
    "\n",
    "                # calling forward propogation\n",
    "                self.forward_prop(xk)\n",
    "                \n",
    "                # cost function \n",
    "                two = self.C(self.a[-1], yk)\n",
    "\n",
    "                num_db = (one - two) / (2 * EPS)\n",
    "                \n",
    "                # move to old value of bias and update the current bias value\n",
    "                self.b[ell][ii] = old_bias_value\n",
    "                \n",
    "                rel_db = np.abs(true_db-num_db)/np.abs(true_db)\n",
    "                print(\"b[{:d}][{:d}]:   true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, true_db, num_db, rel_db))\n",
    "                rel_errors.append(rel_db)\n",
    "\n",
    "        return rel_errors\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        report = []\n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                # TODO: back prop to get derivatives \n",
    "                \n",
    "                self.back_prop(X_train[ind], y_train[ind])\n",
    "                \n",
    "                # TODO: update weights and biases \n",
    "                \n",
    "                # updating bias\n",
    "                self.b = self.b - eta * np.array(self.db)\n",
    "                \n",
    "                # uodating weight\n",
    "                self.W = self.W - eta * (np.array(self.dW) + lam * np.array(self.W))\n",
    "                \n",
    "\n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep + 1) % 5) == 1:\n",
    "                self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "    \n",
    "            if ((ep + 1) % 5) == 1:\n",
    "                r = {\"epoch\" : ep + 1, \"train\" : self.accuracy(X_train, y_train)}\n",
    "                if X_valid is not None:\n",
    "                    r['valid'] = self.accuracy(X_valid, y_valid)\n",
    "                report.append(r)\n",
    "    \n",
    "        # print final accuracy\n",
    "        \n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "        r = {\"epoch\" : ep + 1, \"train\" : self.accuracy(X_train, y_train)}\n",
    "        \n",
    "        # 5th test case\n",
    "        if X_valid is not None:\n",
    "            r['valid'] = self.accuracy(X_valid, y_valid)\n",
    "        report.append(r)\n",
    "        \n",
    "        return report\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "        else: print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the `forward_prop` function in the `Network` class to implement forward propagation.  Your function should take in a single training example `x` and propagate it forward in the network, setting the activations and activities on the hidden and output layers.  When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:46:58.761543Z",
     "start_time": "2018-04-20T02:46:58.749250Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testForwardProp (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `predict` function in the `Network` class to take in a matrix of features and return a matrix of one-hot-encoded label predictions. Your one-hot-encoded predictions should correspond to the output neuron with the largest activation.   \n",
    "\n",
    "When you think your `predict` function is working well, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:01.982781Z",
     "start_time": "2018-04-20T02:47:01.974911Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testPredict (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: OK, now it's time to implement back propagation.  Complete the function ``back_prop`` in the ``Network`` class to use a single training example to compute the derivatives of the loss function with respect to the weights and the biases. As in the **Lecture 18** in-class notebook, you may assume that the loss function for a single training example is given by \n",
    "\n",
    "$$\n",
    "C(y, {\\bf a}^L) = \\frac{1}{2}\\|y - {\\bf a}^L\\|^2  \n",
    "$$\n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:04.552132Z",
     "start_time": "2018-04-20T02:47:04.543799Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testBackProp (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Due to the fact that we hard-code our various activation functions, cost functions, and their derivatives, it is vital to do some debugging to make sure we haven't made a mistake.  \n",
    "\n",
    "One common technique is to do **numerical gradient checking**.  In this method we compute numerical approximations of the derivatives of the cost function with respect to the model parameters and compare them to the analytic versions computed by back prop.  \n",
    "\n",
    "Consider a cost function $C$ which is a function of all of the weights and biases in the network.  We can estimate the derivative of $C$ with respect to a particular parameter using a numerical finite difference technique.  This process looks as follows \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_i} \\approx \\frac{C(w_1,\\ldots, w_i+\\epsilon, \\ldots w_N) - C(w_1,\\ldots, w_i-\\epsilon, \\ldots w_N)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "Evaluating the cost function with the perturbed weights can be accomplished by randomly choosing a training example, performing forward propagation, and then evaluating the cost function using the activations in the output layer.  \n",
    "\n",
    "I've given you starter code down below to do numerical gradient checking.  The code will compute the true and numerical values of the derivative of $C$ with respect to each parameter in the network and then plot the pairs of values as well as their relative errors.  Note that in practice this is extremely expensive, and we typically only check a few random parameters. \n",
    "\n",
    "When you believe your code is correct, you can test it by executing the following cell. Note that a good rule of thumb is to train the network for a handful of epochs before doing the gradient checking, to avoid any transient behavior that might occur at the very beginning of the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:05.949886Z",
     "start_time": "2018-04-20T02:47:05.941572Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testGradCheck (__main__.TestNN) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0][0,0]: true: -1.6129176665e-02  approx: -1.6129176655e-02 rel_err:  6.2758809415e-10\n",
      "W[0][0,1]: true: -3.2258353331e-02  approx: -3.2258353251e-02 rel_err:  2.4817827221e-09\n",
      "b[0][0]:   true: -1.6129176665e-02  approx: -1.6129176655e-02 rel_err:  6.2758809415e-10\n",
      "W[0][1,0]: true:  1.0259238746e-02  approx:  1.0259238739e-02 rel_err:  6.6307893984e-10\n",
      "W[0][1,1]: true:  2.0518477492e-02  approx:  2.0518477439e-02 rel_err:  2.5974560468e-09\n",
      "b[0][1]:   true:  1.0259238746e-02  approx:  1.0259238739e-02 rel_err:  6.6307893984e-10\n",
      "W[0][2,0]: true: -5.1903568015e-04  approx: -5.1903567999e-04 rel_err:  3.0985203531e-10\n",
      "W[0][2,1]: true: -1.0380713603e-03  approx: -1.0380713582e-03 rel_err:  2.0477984656e-09\n",
      "b[0][2]:   true: -5.1903568015e-04  approx: -5.1903567999e-04 rel_err:  3.0985203531e-10\n",
      "W[1][0,0]: true: -7.1372985374e-02  approx: -7.1372985357e-02 rel_err:  2.3383240182e-10\n",
      "W[1][0,1]: true: -4.3055527018e-02  approx: -4.3055527015e-02 rel_err:  8.6212384741e-11\n",
      "W[1][0,2]: true: -7.8283694312e-02  approx: -7.8283694290e-02 rel_err:  2.8537841612e-10\n",
      "b[1][0]:   true: -1.1427975990e-01  approx: -1.1427975983e-01 rel_err:  6.0395790686e-10\n",
      "W[1][1,0]: true:  7.3734308328e-02  approx:  7.3734308308e-02 rel_err:  2.6781078575e-10\n",
      "W[1][1,1]: true:  4.4479987599e-02  approx:  4.4479987595e-02 rel_err:  9.9368482814e-11\n",
      "W[1][1,2]: true:  8.0873653010e-02  approx:  8.0873652984e-02 rel_err:  3.2132963447e-10\n",
      "b[1][1]:   true:  1.1806062206e-01  approx:  1.1806062198e-01 rel_err:  6.8768061143e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.007s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: OK, now let's actually train a neural net!  Complete the missing code in ``train`` to loop over the training data in random order, call `back_prop` to get the derivatives, and then update the weights and the biases via SGD.  When you think you're done, execute the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:05.949886Z",
     "start_time": "2018-04-20T02:47:05.941572Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testSGD (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1E\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Last but not least, we should implement $\\ell$-$2$ regularization.  Modify your `train` function to incorporate regularization of the weights (but **not** the biases) in your SGD update.  As in the Lecture 18 slides, you should assume that the cost function with regularization takes the form \n",
    "\n",
    "$$\n",
    "C_\\lambda = C + \\frac{\\lambda}{2} \\displaystyle\\sum_{w} w^2\n",
    "$$\n",
    "\n",
    "where $\\sum_{w}$ sums over each weight in all layers of the network. Think carefully before you go making large changes to your code.  This modification is much simpler than you think. When you think you're done, execute the following unit test.  (Then go back and execute the test in **Part C** to make sure you didn't break anything.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:07.378983Z",
     "start_time": "2018-04-20T02:47:07.368858Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testRegularizedSGD (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1F\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 2: A Neural Network Classifier for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "In this problem you'll use the Feed-Forward Neural Network framework you wrote in **Problem 1** to take an image of a handwritten digit and predict which digit it corresponds to.  \n",
    "\n",
    "![Samples of Handwritten Digits](mnist.png \"MNIST Digits\")\n",
    "\n",
    "To keep run times down we'll again only consider the subset of the MNIST data set consisting of the digits $3, 7, 8$ and $9$. \n",
    "\n",
    "**Part A**: Executing the following cells will load training and validation data and plot an example handwritten digit.  Explore the training and validation sets and answer the following questions: \n",
    "\n",
    "- How many pixels are in each image in the data set?  \n",
    "- How do the true labels correspond to the associated one-hot-encoded label vectors? \n",
    "- Give an example of a network architecture with a single hidden layer that is compatible with this data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T03:43:07.591473Z",
     "start_time": "2018-04-20T03:43:07.485465Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each image has pixels =  441\n",
      "for 3 = 1000 \n",
      "for 7 = 0100 \n",
      "for 8 = 0010 \n",
      "for 9 = 0001\n",
      "as per my observation, 441 neurons were in input layer and 50 neurons in the hidden layer (may be) and also 4 neurons in the output layer for this network architecture of one hot encoding\n"
     ]
    }
   ],
   "source": [
    "# Load pixel\n",
    "X_train, y_train, X_valid, y_valid = pickle.load(gzip.open(\"../data/mnist21x21_3789_one_hot.pklz\", \"rb\"))\n",
    "print(\"Each image has pixels = \", X_train.shape[1])\n",
    "\n",
    "print(\"for 3 = 1000 \\nfor 7 = 0100 \\nfor 8 = 0010 \\nfor 9 = 0001\")\n",
    "\n",
    "print(\"as per my observation, 441 neurons were in input layer and 50 neurons in the hidden layer (may be) and also 4 neurons in the output layer for this network architecture of one hot encoding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T03:43:09.330860Z",
     "start_time": "2018-04-20T03:43:09.239140Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCI2CIzQKjtD+D7QQyEKBqOIUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2da569426a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_digit(x, label=None):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.imshow(x.reshape(21,21), cmap='gray');\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    if label: plt.xlabel(\"true: {}\".format(label), fontsize=16)\n",
    "        \n",
    "training_index = 2\n",
    "label_dict = dict({0:3, 1:7, 2:8, 3:9})\n",
    "view_digit(X_train[training_index], label_dict[np.argmax(y_train[training_index])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Train a network with a single hidden layer containing $30$ neurons on the first $500$ training examples in the training set using a learning rate of $\\eta = 0.01$ for at least $50$ epochs.  What accuracy does your network achieve on the validation set?  Do you see any clear signs of overfitting?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/150:   train acc:    0.128  valid acc:    0.139\n",
      "epoch   6/150:   train acc:    0.366  valid acc:    0.376\n",
      "epoch  11/150:   train acc:    0.478  valid acc:    0.480\n",
      "epoch  16/150:   train acc:    0.572  valid acc:    0.542\n",
      "epoch  21/150:   train acc:    0.622  valid acc:    0.591\n",
      "epoch  26/150:   train acc:    0.656  valid acc:    0.615\n",
      "epoch  31/150:   train acc:    0.692  valid acc:    0.651\n",
      "epoch  36/150:   train acc:    0.712  valid acc:    0.678\n",
      "epoch  41/150:   train acc:    0.742  valid acc:    0.703\n",
      "epoch  46/150:   train acc:    0.764  valid acc:    0.718\n",
      "epoch  51/150:   train acc:    0.776  valid acc:    0.728\n",
      "epoch  56/150:   train acc:    0.788  valid acc:    0.738\n",
      "epoch  61/150:   train acc:    0.806  valid acc:    0.747\n",
      "epoch  66/150:   train acc:    0.816  valid acc:    0.758\n",
      "epoch  71/150:   train acc:    0.832  valid acc:    0.769\n",
      "epoch  76/150:   train acc:    0.842  valid acc:    0.774\n",
      "epoch  81/150:   train acc:    0.852  valid acc:    0.782\n",
      "epoch  86/150:   train acc:    0.858  valid acc:    0.787\n",
      "epoch  91/150:   train acc:    0.864  valid acc:    0.790\n",
      "epoch  96/150:   train acc:    0.874  valid acc:    0.795\n",
      "epoch 101/150:   train acc:    0.878  valid acc:    0.798\n",
      "epoch 106/150:   train acc:    0.880  valid acc:    0.803\n",
      "epoch 111/150:   train acc:    0.880  valid acc:    0.808\n",
      "epoch 116/150:   train acc:    0.888  valid acc:    0.814\n",
      "epoch 121/150:   train acc:    0.892  valid acc:    0.816\n",
      "epoch 126/150:   train acc:    0.894  valid acc:    0.818\n",
      "epoch 131/150:   train acc:    0.904  valid acc:    0.824\n",
      "epoch 136/150:   train acc:    0.906  valid acc:    0.828\n",
      "epoch 141/150:   train acc:    0.908  valid acc:    0.828\n",
      "epoch 146/150:   train acc:    0.914  valid acc:    0.828\n",
      "epoch 150/150:   train acc:    0.916  valid acc:    0.831\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a04374160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn = Network([441,30,4])\n",
    "n_epochs = 150\n",
    "res = nn.train(X_train[:500], y_train[:500], X_valid, y_valid, eta=0.01, num_epochs=n_epochs)\n",
    "\n",
    "plt.plot([k['epoch'] for k in res], \n",
    "         [k['train'] for k in res], color=\"blue\", label=\"train\")\n",
    "plt.plot([k['epoch'] for k in res], \n",
    "         [k['valid'] for k in res], color=\"green\", label=\"valid\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Accuracy vs epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and validation set accuracy are increasing and I couldn't find any overfitting for more than 30  to 35 neurons and it's keep on increasing with overfitting. I run this model for 150 neurons and seems good with overfitting case for both training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Modify the `Network` class so that it stores the accuracies on the training and validation data every $5$ epochs during the training process. Now increase the number of neurons in the hidden layer to $100$.  On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the learning rates $\\eta = 0.01$, $\\eta = 0.25$ and $\\eta = 1.5$.  Which learning rate seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = Network([441,100,4])\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "etas = [0.01, 0.25, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2da5693d6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_accuracy = 0\n",
    "for i, eta in enumerate(etas):\n",
    "    report = nn.train(X_train, y_train, X_valid, y_valid, eta=eta, num_epochs=80, isPrint=False)\n",
    "    x = [r['epoch'] for r in report] \n",
    "    y = [r['valid'] for r in report]\n",
    "    max_valid = max([r['valid'] for r in report])\n",
    "    max_accuracy = max(max_accuracy, max_valid)\n",
    "    plt.plot(x, y, color=colors[i], label='eta={}'.format(eta))\n",
    "    plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.title(\"accuracy vs eta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph,\n",
    "- eta 0.01 : didn't workout very well because of learning rate is low and took time to converge.\n",
    "- eta 1.5 : It worked well and indicated me the high learning rate because it didn't perform value on the validation set\n",
    "- eta 0.25 : worked best among the three choices we had and gave me a good performance consistently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy on validation set among all ets 0.945\n"
     ]
    }
   ],
   "source": [
    "print(\"Best accuracy on validation set among all ets\",  max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**:  Now let's see if we can get better results with regularization. Using the best learning rate you found in **Part C**, on a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the regularization strengths $\\lambda = 10^{-6}$, $\\lambda = 10^{-4}$ and $\\lambda = 10^{-2}$.  Which regularization strength seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = Network([441,100,4])\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "lams = [0.000001, 0.0001, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlhSMMcb4WFIwxhjjY0nBGGOMz/8DkPfc/O/9XLgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a044014e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_accuracy = 0\n",
    "for i, lam in enumerate(lams):\n",
    "    report = nn.train(X_train, y_train, X_valid, y_valid, eta=0.25, lam=lam, num_epochs=80, isPrint=False)\n",
    "    x = [r['epoch'] for r in report]\n",
    "    y = [r['valid'] for r in report]\n",
    "    max_valid = max([r['valid'] for r in report])\n",
    "    max_accuracy = max(max_accuracy, max_valid)\n",
    "    plt.plot(x, y, color=colors[i], label='lam={}'.format(lam))\n",
    "    plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.title(\"accuracy vs lam\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, on validation set:\n",
    "\n",
    "- lam = 0.01 : worked very badly \n",
    "- lam = 0.0001 : worked well and  higher accuracy was achieved on validation set\n",
    "- lam = 0.000001 : worked almost same as lam 0.0001 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy on validation set =  0.953\n"
     ]
    }
   ],
   "source": [
    "print(\"Best accuracy on validation set = \",  max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**:  Now let's see if we can get better results with different network architectures. On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the architecture from **Part D** as well as two other architectures.  Which architecture seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "nn1 = Network([441,100,4])\n",
    "nn2 = Network([441,40,4])\n",
    "nn3 = Network([441,100, 50, 20, 4])\n",
    "max_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a043746a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, n in enumerate([nn1, nn2, nn3]):\n",
    "    report = n.train(X_train, y_train, X_valid, y_valid, eta=0.25, lam=0.0001, num_epochs=100, isPrint=False)\n",
    "    x = [r['epoch'] for r in report]\n",
    "    y = [r['valid'] for r in report]\n",
    "    max_valid = max([r['valid'] for r in report])\n",
    "    max_accuracy = max(max_accuracy, max_valid)\n",
    "    plt.plot(x, y, color=colors[i], label='architectire={}'.format(i))\n",
    "    plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.title(\"accuracy vs architecture\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the architectures are doing pretty good \n",
    "\n",
    "- Architecture 0 : It has more bumps but also doing pretty well with epochs on validation set\n",
    "- Architecture 1 : It's smoother than architecture 1 and doing pretty well with epochs on validation set\n",
    "- Architecture 2 : It has more bumps than rest of the two but doing pretty well with epochs on validation set but achieved higher accuracy on validation set\n",
    "\n",
    "so overall all the architectures are doing well but architecture 0 is giving its best shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy on validation set =  0.962\n"
     ]
    }
   ],
   "source": [
    "print(\"Best accuracy on validation set = \",  max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [max 20 points] Extra Credit: Improving Network Performance \n",
    "***\n",
    "\n",
    "See if you can get better performance by exploring advanced techniques.  Things you might try are: \n",
    "\n",
    "- Implementing **Mini-Batch** Stochastic Gradient Descent \n",
    "- Experimenting with different activation functions (like tanh and **Leaky** ReLU)\n",
    "- Experimenting with different loss functions (like cross-entropy) \n",
    "\n",
    "For more detailed discussion of these techniques it'll be helpful to look at Chapter 3 of [Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html). \n",
    "\n",
    "The amount of extra credit you receive will be proportional to the number of above suggested tasks that you complete.  Further, to receive credit for the tasks you must not only implement, but also provide evidence that you've tuned the network to make it work.  Comment on the performance differences between the original `Network` implementation and your new networks with bells and whistles. \n",
    "\n",
    "**Important Note**: Don't do any of these things in the original `Network` class, because you'll almost certainly break the unit tests.  Copy the `Network` class from above and rename it `BetterNetwork` (or something) and modify the new class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BetterNetwork:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def C(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate the cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        # TODO: Initialize activation on initial layer to x\n",
    "        \n",
    "        \n",
    "        self.a[0] = x\n",
    "        \n",
    "        \n",
    "        for k in range(self.L-1):\n",
    "            m = np.dot(self.W[k],self.a[k]) + self.b[k]\n",
    "            self.a[k+1] = self.g(m)\n",
    "            self.z[k+1] = m   \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "        \n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "        \n",
    "        # TODO: Populate yhat with one-hot-coded predictions\n",
    "        \n",
    "        for k, x in enumerate(X):\n",
    "            \n",
    "            #Callinf forward propogation\n",
    "            self.forward_prop(x)\n",
    "            \n",
    "            # vactorized y\n",
    "            yhat[k] = np.array([1 if a > 0.5 else 0 for a in self.a[-1]]) \n",
    "                \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations \n",
    "        \n",
    "        #Callinf forward propogation\n",
    "        self.forward_prop(x)\n",
    "        \n",
    "        # TODO: compute deltas on output layer \n",
    "        \n",
    "        # Calling derivative of sigmoid function and cost function\n",
    "        self.delta[-1] =  self.g_prime(self.z[-1]) * self.gradC(self.a[-1], y)\n",
    "        \n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for k in range(self.L-2, -1, -1):\n",
    "            t = self.delta[k + 1]\n",
    "            #activation function\n",
    "            p = self.a[k]\n",
    "            #transposing p\n",
    "            Transpose_p = p[:, np.newaxis].T\n",
    "            \n",
    "            # calculating derivatives of weight\n",
    "            self.dW[k] = np.dot(t[:, np.newaxis], Transpose_p)\n",
    "            \n",
    "            # calculating derivative of bias\n",
    "            self.db[k] = self.delta[k + 1]\n",
    "            \n",
    "            \n",
    "            self.delta[k] = self.g_prime(self.z[k]) * np.dot(self.W[k].T, self.delta[k + 1])\n",
    "            \n",
    "        \n",
    "    def gradient_checking(self, X_train, y_train, EPS=0.0001):\n",
    "        \"\"\"\n",
    "        Performs gradient checking on all weights in the \n",
    "        network for a randomly selected training example \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        \"\"\"\n",
    "        # Randomly select a training example \n",
    "        kk = np.random.randint(0,X_train.shape[0])\n",
    "        xk = X_train[kk]\n",
    "        yk = y_train[kk]\n",
    "\n",
    "        # Get the analytic(ish) weights from back_prop \n",
    "        self.back_prop(xk, yk)\n",
    "\n",
    "        # List of relative errors.  Used only for unit testing. \n",
    "        rel_errors = []\n",
    "\n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        # Don't forget that after perturbing the weights\n",
    "        # you'll want to put them back the way they were! \n",
    "        \n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        for ell in range(self.L-1):\n",
    "            for ii in range(self.W[ell].shape[0]):\n",
    "                # Check weights in level W[ell][ii,jj] \n",
    "                for jj in range(self.W[ell].shape[1]):\n",
    "                    \n",
    "                    # TODO true_dW  \n",
    "                    # TODO num_dW  \n",
    "                    \n",
    "                    # calculating old weight on level W[ell][ii,jj] \n",
    "                    old_Weight = self.W[ell][ii, jj]\n",
    "                    \n",
    "                    # calculating true weight on level W[ell][ii,jj] using weight derivative\n",
    "                    true_dW = self.dW[ell][ii, jj]\n",
    "                    \n",
    "                    #Updating weight by adding EPS\n",
    "                    \n",
    "                    self.W[ell][ii, jj] = self.W[ell][ii, jj] + EPS\n",
    "                    \n",
    "                    #calling forward propogation\n",
    "                    self.forward_prop(xk)\n",
    "                    \n",
    "                    one = self.C(self.a[-1], yk)\n",
    "                    \n",
    "                    #updating weight by subtracting EPS\n",
    "                    self.W[ell][ii, jj] = old_Weight - EPS\n",
    "                    \n",
    "                    # forward propogation\n",
    "                    self.forward_prop(xk)\n",
    "                    \n",
    "                    # after forwarding propogation calling cost function\n",
    "                    two = self.C(self.a[-1], yk)\n",
    "                    \n",
    "                    \n",
    "                    num_dW = (one - two) / (2 * EPS)\n",
    "                    \n",
    "                    # saving old weight back\n",
    "                    \n",
    "                    self.W[ell][ii, jj] = old_Weight\n",
    "                    \n",
    "                    rel_dW = np.abs(true_dW-num_dW)/np.abs(true_dW)\n",
    "                    print(\"W[{:d}][{:d},{:d}]: true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, jj, true_dW, num_dW, rel_dW))\n",
    "                    rel_errors.append(rel_dW)\n",
    "                    \n",
    "                # Check bias b[ell][ii]\n",
    "                \n",
    "                # TODO true_db  \n",
    "                # TODO num_db  \n",
    "                \n",
    "                # we will proceed the same way as weight\n",
    "                \n",
    "                # old bias value from b[ell][ii]\n",
    "                \n",
    "                old_bias_value = self.b[ell][ii]\n",
    "                \n",
    "                # true bias by using derivative of bias\n",
    "                true_db = self.db[ell][ii]\n",
    "                \n",
    "\n",
    "                # updating bias value adding existing and EPS\n",
    "                \n",
    "                self.b[ell][ii] = self.b[ell][ii] + EPS\n",
    "                \n",
    "                \n",
    "                # calling forward propogation\n",
    "                self.forward_prop(xk)\n",
    "                \n",
    "                # cost function after forward propogation and added EPS in bias\n",
    "                one = self.C(self.a[-1], yk)\n",
    "\n",
    "                # subtracting EPS from old bias value using EPS\n",
    "                self.b[ell][ii] = old_bias_value - EPS\n",
    "\n",
    "                # calling forward propogation\n",
    "                self.forward_prop(xk)\n",
    "                \n",
    "                # cost function \n",
    "                two = self.C(self.a[-1], yk)\n",
    "\n",
    "                num_db = (one - two) / (2 * EPS)\n",
    "                \n",
    "                # move to old value of bias and update the current bias value\n",
    "                self.b[ell][ii] = old_bias_value\n",
    "                \n",
    "                rel_db = np.abs(true_db-num_db)/np.abs(true_db)\n",
    "                print(\"b[{:d}][{:d}]:   true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, true_db, num_db, rel_db))\n",
    "                rel_errors.append(rel_db)\n",
    "\n",
    "        return rel_errors\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        report = []\n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                # TODO: back prop to get derivatives \n",
    "                \n",
    "                self.back_prop(X_train[ind], y_train[ind])\n",
    "                \n",
    "                # TODO: update weights and biases \n",
    "                \n",
    "                # updating bias\n",
    "                self.b = self.b - eta * np.array(self.db)\n",
    "                \n",
    "                # uodating weight\n",
    "                self.W = self.W - eta * (np.array(self.dW) + lam * np.array(self.W))\n",
    "                \n",
    "\n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep + 1) % 5) == 1:\n",
    "                self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "    \n",
    "            if ((ep + 1) % 5) == 1:\n",
    "                r = {\"epoch\" : ep + 1, \"train\" : self.accuracy(X_train, y_train)}\n",
    "                if X_valid is not None:\n",
    "                    r['valid'] = self.accuracy(X_valid, y_valid)\n",
    "                report.append(r)\n",
    "    \n",
    "        # print final accuracy\n",
    "        \n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "        r = {\"epoch\" : ep + 1, \"train\" : self.accuracy(X_train, y_train)}\n",
    "        \n",
    "        # 5th test case\n",
    "        if X_valid is not None:\n",
    "            r['valid'] = self.accuracy(X_valid, y_valid)\n",
    "        report.append(r)\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def sgd(self, X_train, y_train, minibatch_size):\n",
    "        \n",
    "        for iter in range(n_iter):\n",
    "            print('Iteration {}'.format(iter))\n",
    "\n",
    "            # Randomize data point\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "            for i in range(0, X_train.shape[0], minibatch_size):\n",
    "                \n",
    "                # Get pair of (X, y) of the current minibatch/chunk\n",
    "                \n",
    "                X_train_mini = X_train[i:i + minibatch_size]\n",
    "                y_train_mini = y_train[i:i + minibatch_size]\n",
    "                sgd_step(X_train_mini, y_train_mini)\n",
    "    \n",
    "    def sgd_step(self, X_train, y_train):\n",
    "        \n",
    "        grad = get_minibatch_grad(model, X_train, y_train)\n",
    "        \n",
    "        # Update every parameters in our networks (W1 and W2) using their gradients\n",
    "        \n",
    "        for layer in grad:\n",
    "            # Learning rate: 1e-4\n",
    "            self.delta[layer] += 1e-4 * self.delta[layer]\n",
    "    \n",
    "    def get_minibatch_grad(self, X_train, y_train):\n",
    "        xs, hs, errs = [], [], []\n",
    "\n",
    "        for x, cls_idx in zip(X_train, y_train):\n",
    "            h, y_pred = forward_prop(x)\n",
    "\n",
    "            # Create probability distribution of true label\n",
    "           \n",
    "            y_true = np.zeros(n_class)\n",
    "            \n",
    "            y_true[int(cls_idx)] = 1.\n",
    "\n",
    "            # Compute the gradient of output layer\n",
    "            \n",
    "            err = y_true - y_pred\n",
    "\n",
    "            # Accumulate the informations of minibatch\n",
    "            # x: input\n",
    "            # h: hidden state\n",
    "            # err: gradient of output layer\n",
    "            \n",
    "            xs.append(x)\n",
    "            hs.append(h)\n",
    "            errs.append(err)\n",
    "\n",
    "        # Backprop using the informations we get from the current minibatch\n",
    "        back_prop(np.array(xs), np.array(hs))\n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "        else: print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### couldn't complete it due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
